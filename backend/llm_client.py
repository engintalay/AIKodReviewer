import requests
import json
import time
import logging
from typing import List, Dict, Optional
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter
from models import CodeSnippet

# Logger
logger = logging.getLogger(__name__)


class LMStudioClient:
    """LMStudio OpenAI-compatible API istemcisi"""
    
    def __init__(self, base_url: str = "http://localhost:8000/v1", model: str = "mistral-7b-instruct-v0.3", context_length: int = 4096):
        self.base_url = base_url
        self.model = model
        self.chat_endpoint = f"{base_url}/chat/completions"
        self.models_endpoint = f"{base_url}/models"
        
        # Model context limit
        self.max_context_tokens = context_length
        
        # Session setup with connection pooling
        self.session = requests.Session()
        
        # Proxy'larÄ± devre dÄ±ÅŸÄ± bÄ±rak (Ã¶zellikle corporate networks iÃ§in)
        self.session.trust_env = False
        self.session.proxies = {}
        
        # Retry strategy
        retry_strategy = Retry(
            total=2,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        
        # Adapter configuration  
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=2,
            pool_maxsize=2,
            pool_block=False
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def check_connection(self) -> bool:
        """LMStudio'ya baÄŸlantÄ± kontrolÃ¼ yap"""
        try:
            response = self.session.get(self.models_endpoint, timeout=15)
            return response.status_code == 200
        except requests.exceptions.Timeout:
            print(f"â±ï¸  LMStudio baÄŸlantÄ± zaman aÅŸÄ±mÄ± (15s)")
            return False
        except requests.exceptions.ConnectionError:
            print(f"âŒ LMStudio baÄŸlanamÄ±yor: {self.models_endpoint}")
            return False
        except Exception as e:
            print(f"âš ï¸  LMStudio hata: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """Mevcut modelleri getir"""
        try:
            response = self.session.get(self.models_endpoint, timeout=15)
            if response.status_code == 200:
                data = response.json()
                return [model.get("id", "") for model in data.get("data", [])]
        except requests.exceptions.Timeout:
            print(f"â±ï¸  Model listesi zaman aÅŸÄ±mÄ±")
        except Exception as e:
            print(f"Model listesi alÄ±nÄ±rken hata: {e}")
        return []
    
    def query_with_context(
        self, 
        question: str, 
        code_snippets: List[CodeSnippet],
        max_tokens: int = 500,
        temperature: float = 0.7,
        max_context_chars: int = 8000,
        chat_history: List[Dict] = None
    ) -> tuple[str, float]:
        """Kod konteksti ile sorgu yap (kontekst boyutunu kontrol et)"""
        
        # Konteksti hazÄ±rla (maksimum 8000 karakter)
        context = self._build_context(code_snippets, max_context_chars=max_context_chars)
        
        # Prompt'u oluÅŸtur
        prompt = self._build_prompt(question, context, chat_history)
        
        # API'ya iste gÃ¶nder
        start_time = time.time()
        response_text = self._call_api(prompt, max_tokens, temperature, chat_history)
        elapsed_time = time.time() - start_time
        
        return response_text, elapsed_time
    
    def _estimate_tokens(self, text: str) -> int:
        """Basit token tahmini (1 token â‰ˆ 4 karakter)"""
        return max(1, len(text) // 4)
    
    def _build_context(self, code_snippets: List[CodeSnippet], max_context_chars: int = 10000) -> str:
        """Kod parÃ§acÄ±klarÄ±ndan kontekst oluÅŸtur (kontekst boyutunu sÄ±nÄ±rla)"""
        if not code_snippets:
            return ""
        
        context_parts = []
        total_chars = 0
        
        for snippet in code_snippets:
            snippet_header = f"--- File: {snippet.file_path} (Lines {snippet.start_line}-{snippet.end_line}) ---\n"
            snippet_text = snippet.code
            
            # Kontekst sÄ±nÄ±rÄ±nÄ± aÅŸÄ±yor mu kontrol et
            snippet_size = len(snippet_header) + len(snippet_text) + 10  # +10 boÅŸ satÄ±rlar iÃ§in
            
            if total_chars + snippet_size > max_context_chars:
                # Ã–nemli snippet'larÄ± (metadata, summary) atlamadan Ã¶nce uyar
                if snippet.file_path in ["PROJECT_METADATA", "ELEMENTS_SUMMARY"]:
                    # Bu snippet'larÄ± mutlaka ekle
                    pass
                else:
                    logger.warning(
                        f"âš ï¸  Kontekst boyutu sÄ±nÄ±rÄ±na ulaÅŸÄ±ldÄ±: {total_chars}/{max_context_chars} karakter. "
                        f"Kalan {len(code_snippets) - len(context_parts)//3} snippet atlanÄ±yor."
                    )
                    break
            
            context_parts.append(snippet_header)
            context_parts.append(snippet_text)
            context_parts.append("")
            total_chars += snippet_size
        
        context = "\n".join(context_parts)
        logger.info(
            f"ðŸ“¦ Kontekst hazÄ±rlandÄ±: {len(context_parts)//3} snippet, "
            f"{len(context)} karakter (~{self._estimate_tokens(context)} token)"
        )
        
        return context
    
    def _build_prompt(self, question: str, context: str, chat_history: List[Dict] = None) -> str:
        """Prompt'u oluÅŸtur (token sÄ±nÄ±rÄ±nÄ± kontrol et)"""
        
        # Rezerv tokenleri ayÄ±r (cevap + buffer)
        reserved_tokens = 500  # Cevap iÃ§in
        available_tokens = self.max_context_tokens - reserved_tokens
        
        # Kontekst tokenlerini kontrol et
        context_tokens = self._estimate_tokens(context)
        question_tokens = self._estimate_tokens(question)
        
        # Chat history ekle
        history_text = ""
        if chat_history:
            history_text = "\n\nÃ–NCEKÄ° SOHBET:\n"
            for msg in chat_history[-3:]:  # Son 3 mesaj
                role = "KullanÄ±cÄ±" if msg.get("role") == "user" else "Asistan"
                history_text += f"{role}: {msg.get('content', '')[:200]}\n"
        
        history_tokens = self._estimate_tokens(history_text)
        total_tokens = context_tokens + question_tokens + history_tokens + 100  # +100 prompt template iÃ§in
        
        if total_tokens > self.max_context_tokens:
            logger.warning(
                f"âš ï¸  UYARI: Tahmini token sayÄ±sÄ± aÅŸÄ±yor!\n"
                f"   Kontekst: {context_tokens} token\n"
                f"   Soru: {question_tokens} token\n"
                f"   GeÃ§miÅŸ: {history_tokens} token\n"
                f"   Toplam: {total_tokens} token (Limit: {self.max_context_tokens})\n"
                f"   â†’ Kontekst otomatik olarak kÄ±saltÄ±lÄ±yor..."
            )
        
        if context:
            prompt = f"""AÅŸaÄŸÄ±daki kod parÃ§acÄ±klarÄ±nÄ± ve konteksti dikkate alarak soruya cevap ver.{history_text}

KONTEKST:
{context}

SORU: {question}

CEVAP:"""
        else:
            prompt = f"""Åžu soruya cevap ver:{history_text}

SORU: {question}

CEVAP:"""
        
        # Prompt'u log et
        logger.info(
            f"ðŸ“‹ PROMPT OLUÅžTURULDU:\n"
            f"   â“ Soru: {question[:80]}...\n"
            f"   ðŸ“„ Kontekst: {len(context)} karakter (~{context_tokens} token)\n"
            f"   ðŸ’¬ GeÃ§miÅŸ: {len(chat_history or [])} mesaj (~{history_tokens} token)\n"
            f"   ðŸ“Š Toplam: ~{total_tokens}/{self.max_context_tokens} token"
        )
        logger.debug(f"   ðŸ“ Full Prompt:\n{prompt[:500]}...")
        
        return prompt
    
    def _call_api(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7, chat_history: List[Dict] = None) -> str:
        """LMStudio API'sÄ±nÄ± Ã§aÄŸÄ±r"""
        try:
            messages = []
            
            # Chat history ekle
            if chat_history:
                for msg in chat_history[-3:]:
                    messages.append({
                        "role": msg.get("role", "user"),
                        "content": msg.get("content", "")
                    })
            
            # Mevcut soruyu ekle
            messages.append({
                "role": "user",
                "content": prompt
            })
            
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": False
            }
            
            # Payload'Ä± log et
            logger.info(
                f"ðŸ”— LMStudio API Ã‡AÄžRISI:\n"
                f"   ðŸ“ URL: {self.chat_endpoint}\n"
                f"   ðŸŽ¯ Model: {self.model}\n"
                f"   ðŸŒ¡ï¸  Temperature: {temperature}\n"
                f"   ðŸ“ Max Tokens: {max_tokens}"
            )
            logger.debug(f"   ðŸ“¦ Payload: {json.dumps(payload, indent=2, ensure_ascii=False)[:1000]}")
            
            response = self.session.post(
                self.chat_endpoint,
                json=payload,
                timeout=120
            )
            
            if response.status_code == 200:
                data = response.json()
                if "choices" in data and len(data["choices"]) > 0:
                    response_content = data["choices"][0]["message"]["content"]
                    logger.info(f"âœ… API CevaplandÄ± (Status: {response.status_code})")
                    logger.debug(f"   ðŸ“¤ Response: {response_content[:300]}...")
                    return response_content
            else:
                error_msg = f"API HatasÄ±: {response.status_code} - {response.text}"
                logger.error(f"âŒ {error_msg}")
                return error_msg
        
        except requests.exceptions.Timeout:
            error_msg = "âŒ Hata: Sorgu zaman aÅŸÄ±mÄ± (Timeout - 120 saniye)"
            logger.error(error_msg)
            return error_msg
        except requests.exceptions.ConnectionError:
            error_msg = "âŒ Hata: LMStudio'ya baÄŸlanÄ±lamÄ±yor. LÃ¼tfen LMStudio'yu baÅŸlattÄ±ÄŸÄ±nÄ±zdan emin olun."
            logger.error(error_msg)
            return error_msg
        except Exception as e:
            error_msg = f"âŒ Hata: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return error_msg
    
    def extract_references_from_response(
        self,
        question: str,
        code_elements: List[Dict],
        response: str
    ) -> List[Dict]:
        """Cevaptan ilgili kod referanslarÄ±nÄ± Ã§Ä±kart"""
        
        references = []
        response_lower = response.lower()
        
        for element in code_elements:
            element_name_lower = element.get("name", "").lower()
            file_path = element.get("file_path", "")
            
            # Basit keyword matching
            if element_name_lower and element_name_lower in response_lower:
                references.append({
                    "file": file_path,
                    "element": element.get("name"),
                    "type": element.get("type"),
                    "lines": [element.get("start_line"), element.get("end_line")]
                })
        
        return references
